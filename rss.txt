é—®é¢˜1ï¼šç‚¹å‡»ä¸‹è½½ä¼šæŠ¥é”™
  > Exception in Tkinter callback
  > Traceback (most recent call last):
  > File "C:\Users\forma\AppData\Local\Programs\Python\Python311\Lib\tkinter\__init__.py", line 1948, in __call__
  > return self.func(*args)
  > ^^^^^^^^^^^^^^^^
  > File "F:\pytest\rssgui.py", line 664, in download_selected
  > selected_urls.append(entry['download_url'])
  > ~~~~~^^^^^^^^^^^^^^^^
  > KeyError: 'download_url'


import tkinter as tk
from tkinter import ttk, messagebox
import feedparser
import requests
import re
from threading import Thread
from io import BytesIO
from urllib.parse import unquote
from PIL import Image, ImageTk
import hashlib
import json
import os
from datetime import datetime

# ====== é…ç½®åŒº ======
QB_LOGIN_URL = "http://192.168.50.3:8080/api/v2/auth/login"
QB_ADD_TORRENT_URL = "http://192.168.50.3:8080/api/v2/torrents/add"
USERNAME = "admin"
PASSWORD = "xxx"  # â† æ›¿æ¢ä¸ºä½ çš„å¯†ç ï¼

PRESET_RSS = {
    "mikanime": {
        "url": "https://mikanime.tv/RSS/MyBangumi?token=2iwXTp1m89Rxj92aJIfIrA%3d%3d",
        "include": r"1æœˆ|æ–°ç•ª|ani",
        "exclude": r"è‹±è¯­|å·´å“ˆ"
    },
    "moe": {
        "url": "https://192.168.50.4/rss.xml",
        "include": r"ä¸­æ–‡",
        "exclude": r"è‹±æ–‡"
    },
}

# ====== å¸¸é‡ ======
IMAGE_WIDTH = 150
IMAGE_HEIGHT = 150
TEXT_WRAP_LENGTH = 800
IMAGE_TIMEOUT = 15
PAGE_SIZE = 50
MAX_DISPLAY_ENTRIES = 200
HISTORY_FILE = "rss.log"


def extract_magnet_links(text):
    if not text:
        return []
    magnets = set()
    magnet_pattern = r'(magnet:\?xt=urn:btih:[a-zA-Z0-9]+(?:&[a-zA-Z0-9%._\-]*)*)'
    for m in re.findall(magnet_pattern, text, re.IGNORECASE):
        magnets.add(m)
    infohash_pattern = r'\b([a-fA-F0-9]{40})\b'
    for h in re.findall(infohash_pattern, text):
        if len(h) == 40:
            magnets.add(f"magnet:?xt=urn:btih:{h.lower()}")
    return list(magnets)


def extract_image_url_from_html(html):
    """ä» HTML å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ª <img src>"""
    if not html:
        return None
    match = re.search(r'<img[^>]+src\s*=\s*["\']([^"\']+)["\'][^>]*>', html, re.IGNORECASE)
    return match.group(1) if match else None


def extract_image_url(item):
    """
    å°è¯•ä» feedparser item ä¸­æå–å›¾ç‰‡ URLï¼š
    1. media:thumbnail (å¸¸è§äº Atom)
    2. content / summary / description (HTML)
    """
    # ä¼˜å…ˆï¼šmedia:thumbnail
    if hasattr(item, 'media_thumbnail') and item.media_thumbnail:
        for thumb in item.media_thumbnail:
            url = thumb.get('url')
            if url:
                return url

    # å…¶æ¬¡ï¼šcontent
    if hasattr(item, 'content'):
        for c in item.content:
            if c.type in ('text/html', 'xhtml', 'application/xhtml+xml'):
                img = extract_image_url_from_html(c.value)
                if img:
                    return img

    # å†æ¬¡ï¼šsummary / description
    for field in ['summary', 'description']:
        value = getattr(item, field, '')
        if value:
            img = extract_image_url_from_html(value)
            if img:
                return img

    return None


def normalize_infohash(magnet):
    match = re.search(r'btih:([a-zA-Z0-9]{32,40})', magnet, re.IGNORECASE)
    if not match:
        return None
    ih = match.group(1).lower()
    if len(ih) == 32:
        # Base32 -> SHA1 hex (40 chars)
        try:
            import base64
            decoded = base64.b32decode(ih.upper())
            ih = decoded.hex()
        except Exception:
            return None
    return ih if len(ih) == 40 else None


def matches_filter(title, summary, include_pat, exclude_pat):
    full_text = (title or "") + " " + (summary or "")
    if exclude_pat.strip():
        try:
            if re.search(exclude_pat, full_text, re.IGNORECASE):
                return False
        except re.error:
            pass
    if include_pat.strip():
        try:
            return bool(re.search(include_pat, full_text, re.IGNORECASE))
        except re.error:
            return False
    return True


def parse_rss_time(time_str):
    if not time_str:
        return "æœªçŸ¥æ—¶é—´"
    for fmt in [
        '%a, %d %b %Y %H:%M:%S %z',
        '%a, %d %b %Y %H:%M:%S %Z',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%d %H:%M:%S',
    ]:
        try:
            dt = datetime.strptime(time_str.strip(), fmt)
            return dt.strftime('%Y-%m-%d %H:%M')
        except ValueError:
            continue
    try:
        parsed = feedparser.datetuple_to_datetime(feedparser._parse_date(time_str))
        return parsed.strftime('%Y-%m-%d %H:%M')
    except Exception:
        pass
    return "æœªçŸ¥æ—¶é—´"


def load_history_by_rss_name(rss_name):
    if not os.path.exists(HISTORY_FILE):
        return {}
    history = {}
    entries_list = []
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    if entry.get('rss_name') == rss_name:
                        entries_list.append(entry)
                except Exception:
                    continue
    except Exception:
        pass

    entries_list.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
    entries_list = entries_list[:MAX_DISPLAY_ENTRIES]

    for e in entries_list:
        ih = e['infohash']
        if ih not in history:
            history[ih] = e
    return history


def save_to_history(new_entries, rss_name):
    existing = {}
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        key = (entry.get('rss_name'), entry.get('infohash'))
                        if key[0] is not None and key[1] is not None:
                            existing[key] = entry
                    except Exception:
                        continue
        except Exception:
            pass

    now_iso = datetime.now().isoformat()
    for e in new_entries:
        key = (rss_name, e['infohash'])
        out_entry = e.copy()
        out_entry['rss_name'] = rss_name
        out_entry['timestamp'] = now_iso
        existing[key] = out_entry

    all_entries = list(existing.values())
    all_entries.sort(key=lambda x: x.get('timestamp', ''), reverse=False)

    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            for e in all_entries:
                f.write(json.dumps(e, ensure_ascii=False) + '\n')
    except Exception:
        pass


def clear_history_for_rss(rss_name, keep=200):
    other_lines = []
    current_rss_entries = []

    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        if entry.get('rss_name') == rss_name:
                            current_rss_entries.append(entry)
                        else:
                            other_lines.append(line)
                    except Exception:
                        other_lines.append(line)
        except Exception:
            pass

    current_rss_entries.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
    kept_entries = current_rss_entries[:keep]

    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            f.writelines(other_lines)
            for e in kept_entries:
                f.write(json.dumps(e, ensure_ascii=False) + '\n')
    except Exception:
        pass


class RSSDownloaderApp:
    def __init__(self, root):
        self.root = root
        self.root.title("RSS ç£åŠ›ä¸‹è½½å™¨")
        self.root.geometry("980x780")
        
        self.current_rss_name = None
        self.all_entries = []  # æ‰€æœ‰æ¡ç›®ï¼ˆå«å†å²ï¼‰
        self.check_vars = {}   # infohash -> BooleanVarï¼ˆä»…å½“å‰é¡µï¼‰
        self.entry_widgets = {}  # infohash -> widget
        self.photo_images = []   # é˜²æ­¢ GC
        self.current_page = 0
        self.selected_infohashes = set()  # æŒä¹…åŒ–é€‰ä¸­çŠ¶æ€ï¼ˆè·¨é¡µï¼‰

        self.create_widgets()

    def create_widgets(self):
        rss_frame = tk.Frame(self.root)
        rss_frame.pack(fill='x', padx=10, pady=5)
        tk.Label(rss_frame, text="RSS:").pack(side='left')
        self.rss_entry = tk.Entry(rss_frame, width=80)
        self.rss_entry.pack(side='left', fill='x', expand=True)
        tk.Button(rss_frame, text="æŸ¥è¯¢", command=self.fetch_rss).pack(side='right', padx=(5, 0))

        preset_frame = tk.Frame(self.root)
        preset_frame.pack(fill='x', padx=10, pady=5)
        for name in PRESET_RSS:
            btn = tk.Button(preset_frame, text=name,
                            command=lambda n=name: self.load_preset(n))
            btn.pack(side='left', padx=5)
        tk.Button(preset_frame, text="æ¸…é™¤å†å²", command=self.clear_history, fg='red').pack(side='right', padx=(5, 0))
        tk.Button(preset_frame, text="æ¸…ç©º", command=self.clear_all).pack(side='right', padx=(5, 0))
        tk.Button(preset_frame, text="å…¨é€‰", command=self.select_all).pack(side='right', padx=(5, 0))

        filter_frame = tk.Frame(self.root)
        filter_frame.pack(fill='x', padx=10, pady=5)
        tk.Label(filter_frame, text="include:").pack(side='left')
        self.include_entry = tk.Entry(filter_frame, width=40)
        self.include_entry.pack(side='left', padx=(5, 10))
        tk.Label(filter_frame, text="exclude:").pack(side='left')
        self.exclude_entry = tk.Entry(filter_frame, width=40)
        self.exclude_entry.pack(side='left', padx=(5, 10))

        ttk.Separator(self.root, orient='horizontal').pack(fill='x', pady=5)

        canvas_frame = tk.Frame(self.root)
        canvas_frame.pack(fill='both', expand=True, padx=10, pady=5)
        self.canvas = tk.Canvas(canvas_frame, highlightthickness=0)
        scrollbar = ttk.Scrollbar(canvas_frame, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = ttk.Frame(self.canvas)
        self.scrollable_frame.bind("<Configure>", 
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw")
        self.canvas.configure(yscrollcommand=scrollbar.set)
        self.canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        self.canvas.bind("<Enter>", self._bind_mousewheel)
        self.canvas.bind("<Leave>", self._unbind_mousewheel)

        btn_frame = tk.Frame(self.root)
        btn_frame.pack(fill='x', padx=10, pady=10)
        tk.Button(btn_frame, text="ä¸‹è½½é€‰ä¸­é¡¹", command=self.download_selected,
                  bg='green', fg='white').pack(side='right')

    def _bind_mousewheel(self, event):
        self.canvas.bind_all("<MouseWheel>", self._on_mousewheel)

    def _unbind_mousewheel(self, event):
        self.canvas.unbind_all("<MouseWheel>")

    def _on_mousewheel(self, event):
        self.canvas.yview_scroll(-1 * (event.delta // 120), "units")

    def reset_scroll_to_top(self):
        self.canvas.yview_moveto(0)

    def load_preset(self, name):
        config = PRESET_RSS[name]
        self.rss_entry.delete(0, tk.END)
        self.rss_entry.insert(0, config["url"])
        self.include_entry.delete(0, tk.END)
        self.include_entry.insert(0, config.get("include", ""))
        self.exclude_entry.delete(0, tk.END)
        self.exclude_entry.insert(0, config.get("exclude", ""))
        self.current_rss_name = name
        self.fetch_rss()
        self.reset_scroll_to_top()

    def select_all(self):
        for infohash in [e['infohash'] for e in self.get_current_page_entries()]:
            self.selected_infohashes.add(infohash)
        self._render_paginated()

    def clear_all(self):
        self.selected_infohashes.clear()
        self._render_paginated()

    def get_current_page_entries(self):
        start = self.current_page * PAGE_SIZE
        end = start + PAGE_SIZE
        return self.all_entries[start:end]

    def fetch_rss(self):
        url = self.rss_entry.get().strip()
        if not url:
            messagebox.showwarning("è­¦å‘Š", "è¯·è¾“å…¥ RSS é“¾æ¥ï¼")
            return

        self.current_rss_name = None
        for name, cfg in PRESET_RSS.items():
            if cfg["url"] == url:
                self.current_rss_name = name
                break
        if self.current_rss_name is None:
            self.current_rss_name = "custom"

        for widget in self.scrollable_frame.winfo_children():
            widget.destroy()
        self.check_vars.clear()
        self.entry_widgets.clear()
        self.photo_images.clear()
        self.all_entries = []

        include_pat = self.include_entry.get()
        exclude_pat = self.exclude_entry.get()
        Thread(target=self._fetch_rss_thread, args=(url, include_pat, exclude_pat), daemon=True).start()

    def _fetch_rss_thread(self, url, include_pat, exclude_pat):
        try:
            feed = feedparser.parse(url)
            if getattr(feed, 'bozo', False) and not feed.entries:
                raise Exception("æ— æ•ˆ RSS æº")

            new_entries = []
            for item in feed.entries:
                title = item.get('title', '').strip()
                if not title:
                    continue

                summary = item.get('summary', '')
                content_text = ''
                if hasattr(item, 'content'):
                    for c in item.content:
                        if c.type in ('text/html', 'xhtml'):
                            content_text = c.value
                            break

                full_desc = (summary + " " + content_text).strip()

                # æŸ¥æ‰¾ magnet æˆ– torrent URL
                magnet = None
                torrent_url = None

                if item.get('link', '').startswith('magnet:'):
                    magnet = item.link
                elif hasattr(item, 'enclosures'):
                    for enc in item.enclosures:
                        href = enc.get('href', '')
                        if href.startswith('magnet:'):
                            magnet = href
                            break
                        elif enc.get('type') == 'application/x-bittorrent':
                            torrent_url = href

                if not magnet and hasattr(item, 'links'):
                    for link in item.links:
                        href = link.get('href', '')
                        if href.startswith('magnet:'):
                            magnet = href
                            break
                        elif link.get('type') == 'application/x-bittorrent':
                            torrent_url = href

                if not magnet:
                    extracted = extract_magnet_links(full_desc)
                    magnet = extracted[0] if extracted else None

                if not magnet and not torrent_url:
                    continue

                # ç»Ÿä¸€ä½¿ç”¨ infohash ä½œä¸ºå”¯ä¸€é”®
                infohash = None
                download_url = None
                if magnet:
                    infohash = normalize_infohash(magnet)
                    download_url = magnet
                elif torrent_url:
                    infohash = hashlib.sha1(torrent_url.encode()).hexdigest()[:40]
                    download_url = torrent_url

                if not infohash:
                    continue

                # æå–å›¾ç‰‡ï¼ˆæ”¯æŒ Atomï¼‰
                image_url = extract_image_url(item)

                pub_time = item.get('published') or item.get('updated') or ''
                formatted_time = parse_rss_time(pub_time)

                should_check = matches_filter(title, full_desc, include_pat, exclude_pat)

                new_entries.append({
                    'infohash': infohash,
                    'title': title,
                    'download_url': download_url,
                    'image_url': image_url,
                    'summary': full_desc,
                    'pub_time': formatted_time,
                    'auto_check': should_check
                })

            save_to_history(new_entries, self.current_rss_name)
            history_dict = load_history_by_rss_name(self.current_rss_name)

            merged = {e['infohash']: e for e in history_dict.values()}
            for e in new_entries:
                merged[e['infohash']] = e

            def sort_key(e):
                matched = matches_filter(e['title'], e.get('summary', ''), include_pat, exclude_pat)
                return (0 if matched else 1, e['title'])

            self.all_entries = sorted(merged.values(), key=sort_key)
            self.current_page = 0
            self.root.after(0, self._render_paginated)
            self.root.after(0, self.reset_scroll_to_top)
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"è§£æå¤±è´¥:\n{str(e)}"))

    def _render_paginated(self):
        for widget in self.scrollable_frame.winfo_children():
            widget.destroy()
        self.check_vars.clear()
        self.entry_widgets.clear()
        self.photo_images.clear()

        page_entries = self.get_current_page_entries()

        if not page_entries:
            tk.Label(self.scrollable_frame, text="æš‚æ— æ•°æ®", fg='gray').pack(pady=20)
            self._show_pagination_controls(total_pages=0)
            return

        for entry in page_entries:
            self.create_entry_widget(entry)

        total_pages = (len(self.all_entries) + PAGE_SIZE - 1) // PAGE_SIZE
        self._show_pagination_controls(total_pages)

    def _show_pagination_controls(self, total_pages):
        for child in self.root.winfo_children():
            if getattr(child, '_is_pagination', False):
                child.destroy()

        if total_pages <= 1:
            return

        frame = tk.Frame(self.root)
        frame._is_pagination = True
        frame.pack(fill='x', padx=10, pady=5)

        tk.Label(frame, text=f"ç¬¬ {self.current_page + 1} / {total_pages} é¡µ").pack(side='left')

        tk.Button(frame, text="â† ä¸Šä¸€é¡µ",
                  command=lambda: self._go_to_page(self.current_page - 1),
                  state='normal' if self.current_page > 0 else 'disabled').pack(side='left', padx=5)

        tk.Button(frame, text="ä¸‹ä¸€é¡µ â†’",
                  command=lambda: self._go_to_page(self.current_page + 1),
                  state='normal' if self.current_page < total_pages - 1 else 'disabled').pack(side='left', padx=5)

    def _go_to_page(self, page):
        self.current_page = page
        self._render_paginated()
        self.reset_scroll_to_top()

    def create_entry_widget(self, entry):
        infohash = entry['infohash']
        frame = tk.Frame(self.scrollable_frame, relief='groove', bd=1, padx=5, pady=5)
        frame.pack(fill='x', pady=4, padx=2)

        var = tk.BooleanVar(value=(infohash in self.selected_infohashes))
        cb = tk.Checkbutton(frame, variable=var, command=lambda ih=infohash: self._on_check_change(ih, var.get()))
        cb.pack(side='left', padx=(0, 10))

        clickable_area = tk.Frame(frame)
        clickable_area.pack(side='left', fill='both', expand=True)

        btn_frame = tk.Frame(clickable_area)
        btn_frame.pack(anchor='ne')
        delete_btn = tk.Button(btn_frame, text="ğŸ—‘ï¸", width=2, command=lambda: self.remove_entry(infohash))
        delete_btn.pack(side='left')
        if entry.get('image_url'):
            retry_btn = tk.Button(btn_frame, text="ğŸ”„", width=2, command=lambda: self._retry_image(infohash))
            retry_btn.pack(side='left')

        img_label = None
        if entry.get('image_url'):
            img_container = tk.Frame(clickable_area, width=IMAGE_WIDTH, height=IMAGE_HEIGHT, bg='#eee')
            img_container.pack_propagate(False)
            img_container.pack(side='left', padx=(0, 10), pady=2)
            img_label = tk.Label(img_container, bg='#eee', text="åŠ è½½ä¸­â€¦", fg='gray')
            img_label.pack(expand=True)
            Thread(target=self._load_image, args=(entry['image_url'], img_label), daemon=True).start()

        text_frame = tk.Frame(clickable_area)
        text_frame.pack(side='left', fill='both', expand=True, padx=(0, 10))

        title_label = tk.Label(
            text_frame,
            text=entry['title'],
            font=('Microsoft YaHei', 12, 'bold'),
            wraplength=TEXT_WRAP_LENGTH,
            justify='left',
            anchor='w'
        )
        title_label.pack(anchor='w', pady=(0, 2))

        time_label = tk.Label(
            text_frame,
            text=entry.get('pub_time', 'æœªçŸ¥æ—¶é—´'),
            font=('Microsoft YaHei', 10),
            fg='gray50',
            wraplength=TEXT_WRAP_LENGTH,
            justify='left',
            anchor='w'
        )
        time_label.pack(anchor='w', pady=(0, 3))

        summary_text = entry.get('summary', '').strip()
        if summary_text:
            clean_summary = re.sub(r'<[^>]+>', '', summary_text)
            summary_label = tk.Label(
                text_frame,
                text=clean_summary,
                font=('Microsoft YaHei', 11),
                fg='gray40',
                wraplength=TEXT_WRAP_LENGTH,
                justify='left',
                anchor='w'
            )
            summary_label.pack(anchor='w')

        def toggle(event):
            widget = event.widget
            current = widget
            while current:
                if current == btn_frame:
                    return
                current = current.master
            new_val = not var.get()
            var.set(new_val)
            self._on_check_change(infohash, new_val)  # ğŸ‘ˆ å…³é”®ï¼šåŒæ­¥ selected_infohashes

        clickable_area.bind("<Button-1>", toggle)
        for child in clickable_area.winfo_children():
            child.bind("<Button-1>", toggle)
            if hasattr(child, 'winfo_children'):
                for grand in child.winfo_children():
                    grand.bind("<Button-1>", toggle)

        self.check_vars[infohash] = var
        self.entry_widgets[infohash] = frame

    def _on_check_change(self, infohash, is_selected):
        if is_selected:
            self.selected_infohashes.add(infohash)
        else:
            self.selected_infohashes.discard(infohash)

    def _load_image(self, url, label):
        try:
            resp = requests.get(url, timeout=IMAGE_TIMEOUT)
            resp.raise_for_status()
            img = Image.open(BytesIO(resp.content))
            img.thumbnail((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)
            tk_img = ImageTk.PhotoImage(img)
            self.root.after(0, lambda: self._set_image(label, tk_img))
        except Exception:
            self.root.after(0, lambda: label.config(text="åŠ è½½å¤±è´¥", fg='red'))

    def _set_image(self, label, tk_img):
        label.config(image=tk_img, text="")
        self.photo_images.append(tk_img)

    def _retry_image(self, infohash):
        frame = self.entry_widgets.get(infohash)
        if not frame:
            return
        for child in frame.winfo_children():
            if isinstance(child, tk.Frame) and child.winfo_reqwidth() == IMAGE_WIDTH:
                for lbl in child.winfo_children():
                    if isinstance(lbl, tk.Label):
                        lbl.config(text="é‡è¯•ä¸­â€¦", image='', fg='gray')
                        entry = next((e for e in self.all_entries if e['infohash'] == infohash), None)
                        if entry and entry.get('image_url'):
                            Thread(target=self._load_image, args=(entry['image_url'], lbl), daemon=True).start()
                        return

    def remove_entry(self, infohash):
        if infohash in self.entry_widgets:
            self.entry_widgets[infohash].destroy()
            del self.entry_widgets[infohash]
        if infohash in self.check_vars:
            del self.check_vars[infohash]
        self.selected_infohashes.discard(infohash)

    def download_selected(self):
        selected_urls = []
        for infohash in self.selected_infohashes:
            entry = next((e for e in self.all_entries if e['infohash'] == infohash), None)
            if entry:
                selected_urls.append(entry['download_url'])

        if not selected_urls:
            messagebox.showwarning("æç¤º", "è¯·å…ˆé€‰æ‹©è¦ä¸‹è½½çš„æ¡ç›®ï¼")
            return

        Thread(target=self._download_links, args=(selected_urls,), daemon=True).start()

    def clear_history(self):
        if not self.current_rss_name:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆé€‰æ‹©ä¸€ä¸ª RSS æºï¼")
            return
        if messagebox.askyesno("ç¡®è®¤", f"å°†ä¿ç•™æœ€æ–°çš„ {MAX_DISPLAY_ENTRIES} æ¡è®°å½•ï¼Œåˆ é™¤æ›´æ—©çš„å†å²ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ"):
            Thread(target=lambda: clear_history_for_rss(self.current_rss_name, MAX_DISPLAY_ENTRIES), daemon=True).start()
            messagebox.showinfo("æç¤º", "å†å²å·²æ¸…ç†ï¼ˆåå°æ‰§è¡Œï¼‰")

    def _download_links(self, urls):
        try:
            session = requests.Session()
            login_data = {"username": USERNAME, "password": PASSWORD}
            session.post(QB_LOGIN_URL, data=login_data, timeout=10)
            for url in urls:
                body = {
                    "urls": url,
                    "savepath": '/Volumes/Storage/download/A',
                    "rename": 'a',
                    "autoTMM": "false"
                }
                session.post(QB_ADD_TORRENT_URL, data=body, timeout=10)
            self.root.after(0, lambda: messagebox.showinfo("æˆåŠŸ", f"{len(urls)} ä¸ªä»»åŠ¡å·²æäº¤ï¼"))
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"ä¸‹è½½å¤±è´¥:\n{str(e)}"))


if __name__ == "__main__":
    root = tk.Tk()
    app = RSSDownloaderApp(root)
    root.mainloop()